{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Project: ...**\n",
        "\n",
        "*Master in Machine Learning for Health, 2023~2024*\n",
        "\n",
        "*Authors: Daniel Corrales, Jaime Fernández & Rafael Rodríguez*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LT3jj61QwJ_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIEhJkSaWW8Z"
      },
      "outputs": [],
      "source": [
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s33bt1hoFviC"
      },
      "outputs": [],
      "source": [
        "#To wrap long text lines\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-LnZYfyY9EX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6f2d2ee6-ca61-40a2-d96b-d6f4dd1ad7e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#For fancy table Display\n",
        "%load_ext google.colab.data_table"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import gensim\n",
        "print(spacy.__version__)"
      ],
      "metadata": {
        "id": "y0qa50z_MKx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_md # Or other"
      ],
      "metadata": {
        "id": "Vc-5iTyzOX6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Data Loading**"
      ],
      "metadata": {
        "id": "ntcDc8IpwklD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "oAsQzSxnwoNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Preprocessing Pipeline**\n",
        "\n",
        "Here we preprocess the corpus to obtain its lemmatized version and we perform N-gram detection."
      ],
      "metadata": {
        "id": "4iWtoBKawrKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.disable('parser')\n",
        "nlp.disable('ner')\n",
        "\n",
        "# Assume that docs are in corpus list\n",
        "# Corpus list contains all documents (train, val and test)\n",
        "lemmatized_corpus = [[tk.lemma_ for tk in nlp(doc.lower()) if (tk.is_alpha or tk.is_digit) \\\n",
        "                      and not tk.is_stop and not tk.is_punct] for doc in corpus]\n",
        "\n",
        "print(f\"Number of documents: {len(lemmatized_corpus)}\")"
      ],
      "metadata": {
        "id": "CngNyLzwwvVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram Detection"
      ],
      "metadata": {
        "id": "CUYGJxGhPLpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Firts step is to find N-grams to improve LDA's performance\n",
        "n_gram_model = gensim.models.phrases.Phrases(lemmatized_corpus, min_count=2, threshold=20)\n",
        "n_gram_corpus = [el for el in n_gram_model[lemmatized_corpues]]\n",
        "\n",
        "# Display table with N-grams\n",
        "n_gram_dict = {}\n",
        "detected_n_grams = []\n",
        "\n",
        "for doc in n_gram_corpus:\n",
        "  for word in doc:\n",
        "    if '_' in word:\n",
        "      if word not in detected_n_grams:\n",
        "        detected_n_grams.append(word)\n",
        "        n_gram_dict[word] = 0\n",
        "\n",
        "      n_gram_dict[word] += 1\n",
        "\n",
        "n_grams_df = pd.DataFrame([(key, value) for key, value in n_gram_dict.items()], columns=['N_gram', 'Count']).sort_values(by='Count', ascending=False)\n",
        "n_grams_df"
      ],
      "metadata": {
        "id": "9X3aNwYiw5m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Vectorization**\n",
        "\n",
        "Three main vectorizations are generated here: BoW, TF-IDF and word embeddings."
      ],
      "metadata": {
        "id": "HP2rvgAKEO02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = gensim.corpora.Dictionary(n_gram_corpus)\n",
        "len_bf = len(D)\n",
        "\n",
        "D.filter_exptremes(no_below=4, no_above=0.8, keep_n=5000)\n",
        "len_af = len(D)\n",
        "\n",
        "print(f\"Dictionary length before filtering: {len_bf}\")\n",
        "print(f\"Dictionary length after filtering: {len_af}\")"
      ],
      "metadata": {
        "id": "0-Z-OQY2EhiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words"
      ],
      "metadata": {
        "id": "ESUAYN8kFO9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bow_sparse = [D.doc2bow(doc) for doc in n_gram_corpus]\n",
        "corpus_bow_dense = gensim.matutils.corpus2dense(corpus_bow_sparse, num_terms=len(D))\n",
        "print(corpus_bow_dense.shape)"
      ],
      "metadata": {
        "id": "mlhykR9PGAmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "rdPLf51aFRB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = gensim.models.TfidfModel(courpus_bow_sparse)\n",
        "corpus_tfidf_sparse = tfidf[corpus_bow_sparse]\n",
        "corpus_tfidf_dense = gensim.matutils.corpus2dense(corpus_tfidf_sparse, num_terms=len(D))"
      ],
      "metadata": {
        "id": "j_w67LejFSlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings"
      ],
      "metadata": {
        "id": "kTGO99SDFS-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "RtKBW9LpFUr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Topic Modeling: Latent Dirichlet Allocation**\n",
        "\n",
        "Latent Dirichlet Allocation is peformed to analyze the sematic structuture of the corpus in terms of topics and to obtain the LDA document vectorization."
      ],
      "metadata": {
        "id": "iCx7cdNxwwJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Dirichlet Allocation with Mallet"
      ],
      "metadata": {
        "id": "wY2vkk9_POWz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjMGZveJZy5E"
      },
      "outputs": [],
      "source": [
        "def install_java():\n",
        "    !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    !java -version\n",
        "install_java()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JhOr21VaFck"
      },
      "outputs": [],
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3B_JVPc1bLum",
        "outputId": "4d91f5cb-7cd7-46d6-e224-a1e4e084bea6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "os.environ['MALLET_HOME'] = 'mallet-2.0.8'\n",
        "mallet_path = 'mallet-2.0.8/bin/mallet'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import corpus to *.txt file."
      ],
      "metadata": {
        "id": "uJM3tBM8Htlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('n_gram_corpus.txt', 'w'):\n",
        "  for i, doc in n_gram_corpus:\n",
        "    f.write(f\"{i} 0 {\" \".join(doc)}\")\n",
        "  f.close()\n",
        "\n",
        "!mallet-2.0.8/bin/mallet import-file --input mycorpus.txt --output mycorpus.mallet --keep-sequence --remove-stopwords"
      ],
      "metadata": {
        "id": "nOqZfVH6PQW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze coherence for different number of topics."
      ],
      "metadata": {
        "id": "PZnPPki4Ie7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mallet_LDA(n_topics, corpus, coherence=True):\n",
        "  # Train LDA\n",
        "  print(f\"Mallet LDA n_topics {n_topics}\")\n",
        "  command = f\"mallet-2.0.8/bin/mallet train-topics --input mycorpus.mallet --num-topics {n_topics} --num-iterations 1000 \\\n",
        "              --output-doc-topics doc_topics.txt --word-topic-counts-file wtc_counts.txt --topic-word-weights-file \\\n",
        "              topic_weights.txt --output-topic-keys topic_keys.txt --num-top-words 20\"\n",
        "  subprocess.call(command, shell=True)\n",
        "\n",
        "  # Retrieve doc-topics matrix and sparsify\n",
        "  thetas = []\n",
        "  with open('doc_topics.txt', 'r') as f:\n",
        "    doc_topic = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "  for line in doc_topic:\n",
        "    digits = line.split(\"\\t\")[2:]\n",
        "    thetas.append(np.array([float(dig) for dig in digits]))\n",
        "\n",
        "  thetas = np.vstack(thetas)\n",
        "  thetas = np.where(thetas < 0.01, 0, thetas)\n",
        "  thetas = [[(i, prob) for i, prob in enumerate(doc) if prob != 0.0] for doc in thetas]\n",
        "\n",
        "  # Create corpus and dictionary in Mallet format\n",
        "  with open(\"topic_keys.txt\") as fin:\n",
        "    tpc_descriptions = fin.readlines()\n",
        "    tpc_descriptions = [el.strip().split(\"\\t\")[-1].split() for el in tpc_descriptions]\n",
        "\n",
        "  Dict = gensim.corpora.Dictionary(corpus)\n",
        "\n",
        "  # Compute coherence\n",
        "  if coherence:\n",
        "    nwords = 10\n",
        "    cm = CoherenceModel(topics=tpc_descriptions, texts=corpus, dictionary=Dict, coherence='c_v', topn=n_words)\n",
        "    coherence = np.mean(cm.get_coherence_per_topic())\n",
        "    return thetas, tpc_descriptions, Dict, coherence\n",
        "\n",
        "  return thetas, tpc_descriptions, Dict"
      ],
      "metadata": {
        "id": "0VZXgqWFLP6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_topics = [5, 10, 15, 20, 25, 50]\n",
        "coherence_list_mallet = []\n",
        "\n",
        "for n in n_topics:\n",
        "  _, _, _, coherence = compute_mallet_LDA(n, corpus=n_gram_corpus, coherence=True)\n",
        "  coherence_list_mallet.append(coherence)"
      ],
      "metadata": {
        "id": "HtkZ4oPIIeK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(n_topics, coherence_list_mallet, label='Mallet', marker='o')\n",
        "plt.xlabel('Number of topics')\n",
        "plt.ylabel('Coherence')\n",
        "plt.title('Mallet LDA coherence')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aX5zQ7rqJNQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrain the model with highest coherence."
      ],
      "metadata": {
        "id": "QCpZew7jMjlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_max = np.argsort(coherence_list_mallet)[-1]\n",
        "best_n_topics = n_topics[idx_max]\n",
        "\n",
        "LDA_corpus, tpc_descriptions, Dict = compute_mallet_LDA(best_n_topics, corpus=n_gram_corpus, coherence=False) # LDA_corpus contains the vectorization"
      ],
      "metadata": {
        "id": "xzVEWRmXMnck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization and Analysis"
      ],
      "metadata": {
        "id": "KLAR6rGJPQyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "kjqEvkQAPS_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Supervised Learning Task**"
      ],
      "metadata": {
        "id": "663D8vjOw62C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classical Models: ..."
      ],
      "metadata": {
        "id": "ZLuZ1vzoErzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "vGK2ZM3bw9OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers"
      ],
      "metadata": {
        "id": "ibyoj2EZEwpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "R_kX2ylzExEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Semantic Distances between Documents**"
      ],
      "metadata": {
        "id": "EB-dByq7E4H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "PEB5gyFIE71L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Extension Work**"
      ],
      "metadata": {
        "id": "Pa6nh7KHNFpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TDOD"
      ],
      "metadata": {
        "id": "1L70GJbXNLTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}