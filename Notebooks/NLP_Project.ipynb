{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT3jj61QwJ_w"
      },
      "source": [
        "# **NLP Project: Analyzing and Predicting Salary from Job Description**\n",
        "\n",
        "*Master in Machine Learning for Health, 2023~2024*\n",
        "\n",
        "*Authors: Daniel Corrales, Jaime Fernández & Rafael Rodríguez*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIEhJkSaWW8Z"
      },
      "outputs": [],
      "source": [
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utXptUwYaZ3G"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s33bt1hoFviC"
      },
      "outputs": [],
      "source": [
        "#To wrap long text lines\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "D-LnZYfyY9EX",
        "outputId": "6f2d2ee6-ca61-40a2-d96b-d6f4dd1ad7e0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# For fancy table Display\n",
        "%load_ext google.colab.data_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0qa50z_MKx0"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import gensim\n",
        "print(spacy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc-5iTyzOX6s"
      },
      "outputs": [],
      "source": [
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_md # Or other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntcDc8IpwklD"
      },
      "source": [
        "## **0. Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAsQzSxnwoNB"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iWtoBKawrKQ"
      },
      "source": [
        "## **1. Preprocessing Pipeline**\n",
        "\n",
        "Here we preprocess the corpus to obtain its lemmatized version and we perform N-gram detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CngNyLzwwvVv"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.disable('parser')\n",
        "nlp.disable('ner')\n",
        "\n",
        "# Assume that docs are in corpus list\n",
        "# Corpus list contains all documents (train, val and test)\n",
        "lemmatized_corpus = [[tk.lemma_ for tk in nlp(doc.lower()) if (tk.is_alpha or tk.is_digit) \\\n",
        "                      and not tk.is_stop and not tk.is_punct] for doc in corpus]\n",
        "\n",
        "print(f\"Number of documents: {len(lemmatized_corpus)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUYGJxGhPLpN"
      },
      "source": [
        "### N-gram Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X3aNwYiw5m6"
      },
      "outputs": [],
      "source": [
        "# Firts step is to find N-grams to improve LDA's performance\n",
        "n_gram_model = gensim.models.phrases.Phrases(lemmatized_corpus, min_count=2, threshold=20)\n",
        "n_gram_corpus = [el for el in n_gram_model[lemmatized_corpues]]\n",
        "\n",
        "# Display table with N-grams\n",
        "n_gram_dict = {}\n",
        "detected_n_grams = []\n",
        "\n",
        "for doc in n_gram_corpus:\n",
        "  for word in doc:\n",
        "    if '_' in word:\n",
        "      if word not in detected_n_grams:\n",
        "        detected_n_grams.append(word)\n",
        "        n_gram_dict[word] = 0\n",
        "\n",
        "      n_gram_dict[word] += 1\n",
        "\n",
        "n_grams_df = pd.DataFrame([(key, value) for key, value in n_gram_dict.items()], columns=['N_gram', 'Count']).sort_values(by='Count', ascending=False)\n",
        "n_grams_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP2rvgAKEO02"
      },
      "source": [
        "## **2. Vectorization**\n",
        "\n",
        "Three main vectorizations are generated here: BoW, TF-IDF and word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-Z-OQY2EhiG"
      },
      "outputs": [],
      "source": [
        "D = gensim.corpora.Dictionary(n_gram_corpus)\n",
        "len_bf = len(D)\n",
        "\n",
        "D.filter_exptremes(no_below=4, no_above=0.8, keep_n=5000)\n",
        "len_af = len(D)\n",
        "\n",
        "print(f\"Dictionary length before filtering: {len_bf}\")\n",
        "print(f\"Dictionary length after filtering: {len_af}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESUAYN8kFO9u"
      },
      "source": [
        "### Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlhykR9PGAmk"
      },
      "outputs": [],
      "source": [
        "corpus_bow_sparse = [D.doc2bow(doc) for doc in n_gram_corpus]\n",
        "corpus_bow_dense = gensim.matutils.corpus2dense(corpus_bow_sparse, num_terms=len(D))\n",
        "print(corpus_bow_dense.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdPLf51aFRB2"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_w67LejFSlj"
      },
      "outputs": [],
      "source": [
        "tfidf = gensim.models.TfidfModel(courpus_bow_sparse)\n",
        "corpus_tfidf_sparse = tfidf[corpus_bow_sparse]\n",
        "corpus_tfidf_dense = gensim.matutils.corpus2dense(corpus_tfidf_sparse, num_terms=len(D))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTGO99SDFS-k"
      },
      "source": [
        "### Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtKBW9LpFUr2"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCx7cdNxwwJU"
      },
      "source": [
        "## **3. Topic Modeling: Latent Dirichlet Allocation**\n",
        "\n",
        "Latent Dirichlet Allocation is peformed to analyze the sematic structuture of the corpus in terms of topics and to obtain the LDA document vectorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY2vkk9_POWz"
      },
      "source": [
        "### Latent Dirichlet Allocation with Mallet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjMGZveJZy5E"
      },
      "outputs": [],
      "source": [
        "def install_java():\n",
        "    !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    !java -version\n",
        "install_java()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JhOr21VaFck"
      },
      "outputs": [],
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3B_JVPc1bLum",
        "outputId": "4d91f5cb-7cd7-46d6-e224-a1e4e084bea6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "os.environ['MALLET_HOME'] = 'mallet-2.0.8'\n",
        "mallet_path = 'mallet-2.0.8/bin/mallet'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJM3tBM8Htlj"
      },
      "source": [
        "Import corpus to *.txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOqZfVH6PQW4"
      },
      "outputs": [],
      "source": [
        "with open('n_gram_corpus.txt', 'w'):\n",
        "  for i, doc in n_gram_corpus:\n",
        "    f.write(f\"{i} 0 {\" \".join(doc)}\")\n",
        "  f.close()\n",
        "\n",
        "!mallet-2.0.8/bin/mallet import-file --input mycorpus.txt --output mycorpus.mallet --keep-sequence --remove-stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZnPPki4Ie7v"
      },
      "source": [
        "Analyze coherence for different number of topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VZXgqWFLP6Y"
      },
      "outputs": [],
      "source": [
        "def compute_mallet_LDA(n_topics, corpus, coherence=True):\n",
        "  # Train LDA\n",
        "  print(f\"Mallet LDA n_topics {n_topics}\")\n",
        "  command = f\"mallet-2.0.8/bin/mallet train-topics --input mycorpus.mallet --num-topics {n_topics} --num-iterations 1000 \\\n",
        "              --output-doc-topics doc_topics.txt --word-topic-counts-file wtc_counts.txt --topic-word-weights-file \\\n",
        "              topic_weights.txt --output-topic-keys topic_keys.txt --num-top-words 20\"\n",
        "  subprocess.call(command, shell=True)\n",
        "\n",
        "  # Retrieve doc-topics matrix and sparsify\n",
        "  thetas = []\n",
        "  with open('doc_topics.txt', 'r') as f:\n",
        "    doc_topic = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "  for line in doc_topic:\n",
        "    digits = line.split(\"\\t\")[2:]\n",
        "    thetas.append(np.array([float(dig) for dig in digits]))\n",
        "\n",
        "  thetas = np.vstack(thetas)\n",
        "  thetas = np.where(thetas < 0.01, 0, thetas)\n",
        "  thetas = [[(i, prob) for i, prob in enumerate(doc) if prob != 0.0] for doc in thetas]\n",
        "\n",
        "  # Create corpus and dictionary in Mallet format\n",
        "  with open(\"topic_keys.txt\") as fin:\n",
        "    tpc_descriptions = fin.readlines()\n",
        "    tpc_descriptions = [el.strip().split(\"\\t\")[-1].split() for el in tpc_descriptions]\n",
        "\n",
        "  Dict = gensim.corpora.Dictionary(corpus)\n",
        "\n",
        "  # Compute coherence\n",
        "  if coherence:\n",
        "    nwords = 10\n",
        "    cm = CoherenceModel(topics=tpc_descriptions, texts=corpus, dictionary=Dict, coherence='c_v', topn=n_words)\n",
        "    coherence = np.mean(cm.get_coherence_per_topic())\n",
        "    return thetas, tpc_descriptions, Dict, coherence\n",
        "\n",
        "  return thetas, tpc_descriptions, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtkZ4oPIIeK6"
      },
      "outputs": [],
      "source": [
        "n_topics = [5, 10, 15, 20, 25, 50]\n",
        "coherence_list_mallet = []\n",
        "\n",
        "for n in n_topics:\n",
        "  _, _, _, coherence = compute_mallet_LDA(n, corpus=n_gram_corpus, coherence=True)\n",
        "  coherence_list_mallet.append(coherence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX5zQ7rqJNQ5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(n_topics, coherence_list_mallet, label='Mallet', marker='o')\n",
        "plt.xlabel('Number of topics')\n",
        "plt.ylabel('Coherence')\n",
        "plt.title('Mallet LDA coherence')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCpZew7jMjlN"
      },
      "source": [
        "Retrain the model with highest coherence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzVEWRmXMnck"
      },
      "outputs": [],
      "source": [
        "idx_max = np.argsort(coherence_list_mallet)[-1]\n",
        "best_n_topics = n_topics[idx_max]\n",
        "\n",
        "LDA_corpus, tpc_descriptions, Dict = compute_mallet_LDA(best_n_topics, corpus=n_gram_corpus, coherence=False) # LDA_corpus contains the vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLAR6rGJPQyc"
      },
      "source": [
        "### Visualization and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjqEvkQAPS_q"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "663D8vjOw62C"
      },
      "source": [
        "## **4. Supervised Learning Task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLuZ1vzoErzh"
      },
      "source": [
        "### Classical Models: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGK2ZM3bw9OP"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibyoj2EZEwpu"
      },
      "source": [
        "### Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_kX2ylzExEb"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB-dByq7E4H5"
      },
      "source": [
        "## **5. Semantic Graph and Distances between Documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLq3ZV56k9mP"
      },
      "source": [
        "Convert to sparse matrix representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEB5gyFIE71L"
      },
      "outputs": [],
      "source": [
        "# Convert from list of tuples to sparse matrix\n",
        "# TODO: LDA_corpus\n",
        "\n",
        "LDA_sparse = corpus2csc(LDA_corpus).T\n",
        "n_topics = LDA_sparse.shape[1]\n",
        "n_docs = LDA_sparse.shape[0]\n",
        "\n",
        "print(f\"Number of topics: {n_topics}\")\n",
        "print(f\"X: sparse matrix with {LDA_sparse.nnz} nonzero values out of {n_docs * n_topics}\")\n",
        "print(LDA_sparse.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x89tC8AlCJ1"
      },
      "source": [
        "Renormalize rows to get probabilistic embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auoP6gqek5Ze"
      },
      "outputs": [],
      "source": [
        "print(f\"Average row sum before normalizing: {np.mean(LDA_sparse.sum(axis=1).T)}\")\n",
        "\n",
        "LDA_sparse = scsp.csr_matrix(LDA_sparse / np.sum(LDA_sparse, axis=1))\n",
        "\n",
        "print(f\"Average row sum after normalizing: {np.mean(LDA_sparse.sum(axis=1).T)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLV8FzkdlhP4"
      },
      "source": [
        "Compute Similarity matrix based on Battacharyya Coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHLKensTlolw"
      },
      "outputs": [],
      "source": [
        "S = np.sqrt(LDA_sparse) * np.sqrt(LDA_sparse.T)\n",
        "\n",
        "print(f\"Shape of S: {S.shape}\")\n",
        "print(f\"Number of non-zero elements: {S.nnz}\")\n",
        "print(f\"Proportion of non-zero values: {S.nnz / (S.shape[0] * S.shape[1]) * 100:.3f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rFRiA0BlwJm"
      },
      "outputs": [],
      "source": [
        "S = scsp.triu(S, k=1) # Mantain upper triangular matrix\n",
        "print('Number of non-zero components in S:', S.nnz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3CKrJIzl-Fw"
      },
      "source": [
        "Threshold similarity matrix to discard non important edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0bHToudl0zW"
      },
      "outputs": [],
      "source": [
        "n_nodes = LDA_sparse.shape[0]\n",
        "n_edges = S.nnz\n",
        "n_edges_per_node = n_edges / n_nodes\n",
        "\n",
        "print(f\"Number of nodes: {n_nodes}\")\n",
        "print(f\"Number of edges: {n_edges}\")\n",
        "print(f\"Number of edges per node: {n_edges_per_node}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqIz6ljLmO59"
      },
      "outputs": [],
      "source": [
        "n_edges_per_node = 10\n",
        "\n",
        "n_edges = n_edges_per_node * n_nodes\n",
        "sorted_sim = np.sort(S.data)[::-1]\n",
        "thr = sorted_sim[n_edges]\n",
        "\n",
        "print(f\"Threshold: {thr}\")\n",
        "print(f\"Number of edges: {n_edges}\")\n",
        "\n",
        "plt.figure(figsize=(8, 2))\n",
        "plt.hist(S.data, bins=100)\n",
        "plt.axvline(thr, 'r-')\n",
        "plt.xlabel('Similarity values')\n",
        "plt.ylabel('No. of matrix entries')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dxq7XPnmgPq"
      },
      "outputs": [],
      "source": [
        "S.data = np.where(S.data < thr, 0, S.data)\n",
        "S.eliminate_zeros()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7uOdbTVmxuX"
      },
      "source": [
        "Plot graph and compute layout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StoSPu0Zm1TR"
      },
      "outputs": [],
      "source": [
        "G = nx.from_scipy_sparse_array(S)\n",
        "positions = nx.spring_layout(G, iterations=50, seed=0)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "nx.draw(G, positions, node_size=2, width=0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9BP9n9unE8o"
      },
      "source": [
        "Find connected components and plot them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsRnCLwBnITR"
      },
      "outputs": [],
      "source": [
        "nodes_lcc = list(nx.connected_components(G))[0]\n",
        "n_conn_comp = len(nodes_lcc)\n",
        "\n",
        "fig, axes = plt.subplots(n_conn_comp, 1, figsize=(15,15), layout='tight')\n",
        "\n",
        "for i, conn_comp in enumerate(nodes_lcc):\n",
        "  G_lcc = G.subgraph(conn_comp)\n",
        "  positions_lcc = nx.spring_layout(G_lcc, iterations=50, seed=0)\n",
        "\n",
        "  nx.draw(G_lcc, positions_lcc, node_size=2, width=0.06, ax=axes[i])\n",
        "  axes[i].set_title(f\"Connected Component {i+1}\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-HJhLroGxf"
      },
      "source": [
        "Community detection algorithm to explore semantic structure of corpus and similarities between documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_BucqnNoXyO"
      },
      "outputs": [],
      "source": [
        "C = nx_comm.louvain_communities(G, seed=0)\n",
        "\n",
        "# Metrics of the partition\n",
        "modularity = nx_comm.modularity(G, C)\n",
        "coverage, performance = nx_comm.partition_quality(G, C)\n",
        "\n",
        "nc = len(C)\n",
        "print(f\"Number of communities: {nc}\")\n",
        "print(f\"Modularity: {modularity}\")\n",
        "print(f\"Coverage: {coverage}\")\n",
        "print(f\"Performance: {performance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIjZCJuVomY0"
      },
      "outputs": [],
      "source": [
        "palette = sns.color_palette(palette=\"Paired\", n_colors=nc)\n",
        "\n",
        "node2comm = {n: 0 for n in G}\n",
        "for i, com in enumerate(C):\n",
        "    for node in list(com):\n",
        "        node2comm[node] = i\n",
        "\n",
        "# Map node attribute to rgb colors\n",
        "node_colors = [palette[node2comm[n]] for n in G]\n",
        "\n",
        "# Get list of degrees\n",
        "degrees = [val / 3 for (node, val) in G.degree()]\n",
        "\n",
        "#  Draw graph\n",
        "plt.figure(figsize=(7, 7))\n",
        "nx.draw(G, positions, node_size=degrees, node_color=node_colors, width=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M4bpwnqo-xQ"
      },
      "source": [
        "Analyze communities in terms of topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NeEx6kspCia"
      },
      "outputs": [],
      "source": [
        "# TODO: Personalize with our implementation\n",
        "\n",
        "# col = 'main_topic_35'\n",
        "# newcols=['Community', 'Main topic', 'Topic 2', 'Topic 3', 'Topic 4']\n",
        "\n",
        "# df = pd.DataFrame(columns=newcols).set_index('Community')\n",
        "\n",
        "# # For each community...\n",
        "# for i, docs in enumerate(C):\n",
        "\n",
        "#     # Join all keywords from all documents in the community\n",
        "#     words = \",\".join(NSF_df.iloc[list(docs)][col]).split(\",\")\n",
        "\n",
        "#     # Count the number of occurences of each keyword\n",
        "#     kwd_count = Counter(words)\n",
        "#     n_votes = sum(kwd_count.values())\n",
        "\n",
        "#     # Keep keywords that are frequent enough\n",
        "#     kwds = [k for k, v in kwd_count.items() if v > 0.12 * n_votes]\n",
        "#     kwds = sorted(kwds, key= lambda x: kwd_count[x], reverse=True)\n",
        "#     kwds = [f'C{i}'] + kwds + [\"\"] * (len(newcols) - len(kwds) -1)\n",
        "\n",
        "#     row = pd.Series(dict(zip(newcols, kwds))).to_frame().T.set_index('Community')\n",
        "#     df = pd.concat([df, row], axis=0)\n",
        "\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa6nh7KHNFpi"
      },
      "source": [
        "## **6. Extension Work**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L70GJbXNLTX"
      },
      "outputs": [],
      "source": [
        "# TDOD"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ntcDc8IpwklD",
        "4iWtoBKawrKQ",
        "HP2rvgAKEO02",
        "663D8vjOw62C"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
